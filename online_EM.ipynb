{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import multivariate_normal\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(998, 1536)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('./fineweb_dataset_embeded.npy')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducted\n",
    "# n = dataset.shape[0]\n",
    "\n",
    "# def compute_posterior(x, mu, cov, weights):\n",
    "#     n_centers = len(mu)\n",
    "#     posterior = np.zeros(n_centers)\n",
    "#     for i in range(n_centers):\n",
    "#         posterior[i] = weights[i] * multivariate_normal.pdf(x, mu[i], cov[i])\n",
    "#     return np.sum(posterior)\n",
    "\n",
    "# # Hyperparameters\n",
    "# T = 5\n",
    "# n_centers = 3\n",
    "\n",
    "# # Initialization\n",
    "# gaussian_dist = GaussianMixture(n_components = n_centers)\n",
    "# gaussian_dist.fit(dataset)\n",
    "\n",
    "# means = gaussian_dist.means_\n",
    "# covariances = gaussian_dist.covariances_\n",
    "# weights = gaussian_dist.weights_\n",
    "\n",
    "# si = np.zeros(n)\n",
    "# global_mu = np.sum(si, axis=0)\n",
    "\n",
    "# # Iterations\n",
    "# for t in range(T):\n",
    "#     order = np.random.permutation(n)\n",
    "#     for i in order:\n",
    "#         s0_i = compute_posterior(dataset[i], means, covariances, weights)\n",
    "#         global_mu += s0_i - si[i]\n",
    "#         si[i] = s0_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-step: Compute responsibilities (posterior probabilities) for each data point\n",
    "def e_step(X, mu, covariances, weights, n_components):\n",
    "    n_samples = X.shape[0]\n",
    "    responsibilities = np.zeros((n_samples, n_components))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        total_prob = 0.0\n",
    "        # Calculate the probability for each component and normalize to get responsibilities\n",
    "        for k in range(n_components):\n",
    "            pdf_val = multivariate_normal.pdf(X[i], mean=mu[k], cov=covariances[k])\n",
    "            responsibilities[i, k] = weights[k] * pdf_val\n",
    "            total_prob += responsibilities[i, k]\n",
    "\n",
    "        # Normalize to get the posterior probabilities (responsibilities)\n",
    "        responsibilities[i, :] /= total_prob\n",
    "    \n",
    "    return responsibilities\n",
    "\n",
    "# M-step: Update the parameters of the GMM based on the responsibilities\n",
    "def m_step(X, responsibilities, mu, covariances, weights, n_components):\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Update the parameters incrementally\n",
    "    for k in range(n_components):\n",
    "        # Compute the total responsibility for component k\n",
    "        total_responsibility = np.sum(responsibilities[:, k])\n",
    "\n",
    "        # Update the weight (prior) for component k\n",
    "        weights[k] = total_responsibility / n_samples\n",
    "\n",
    "        # Update the mean (mu) for component k (weighted average of the data points)\n",
    "        mu[k] = np.sum(responsibilities[:, k][:, np.newaxis] * X, axis=0) / total_responsibility\n",
    "\n",
    "        # Update the covariance matrix (weighted covariance)\n",
    "        cov_matrix = np.zeros((n_features, n_features))\n",
    "        for i in range(n_samples):\n",
    "            diff = X[i] - mu[k]\n",
    "            cov_matrix += responsibilities[i, k] * np.outer(diff, diff)\n",
    "\n",
    "        covariances[k] = cov_matrix / total_responsibility\n",
    "\n",
    "    return mu, covariances, weights\n",
    "\n",
    "# Incremental EM Algorithm\n",
    "def incremental_em(X, n_components, max_iter=100, tol=1e-6):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize the GMM parameters (means, covariances, and weights)\n",
    "    np.random.seed(42)\n",
    "    mu = np.random.randn(n_components, n_features)\n",
    "    covariances = np.array([np.eye(n_features)] * n_components)  # Identity covariance for each component\n",
    "    weights = np.ones(n_components) / n_components  # Equal weight for each component initially\n",
    "    \n",
    "    # Initialize responsibilities (posterior probabilities) for each data point\n",
    "    responsibilities = np.zeros((n_samples, n_components))\n",
    "\n",
    "    # Iterative process\n",
    "    prev_log_likelihood = -np.inf\n",
    "    for t in range(max_iter):\n",
    "        print(f\"Iteration {t + 1}/{max_iter}\")\n",
    "\n",
    "        # E-step: Compute responsibilities for each data point\n",
    "        responsibilities = e_step(X, mu, covariances, weights, n_components)\n",
    "\n",
    "        # M-step: Update the parameters of the GMM\n",
    "        mu, covariances, weights = m_step(X, responsibilities, mu, covariances, weights, n_components)\n",
    "\n",
    "        # Compute the log-likelihood for convergence check\n",
    "        log_likelihood = np.sum(np.log(np.sum(responsibilities, axis=1)))\n",
    "        print(f\"Log-likelihood at iteration {t + 1}: {log_likelihood}\")\n",
    "\n",
    "        # Check for convergence (if log-likelihood changes by less than tol, stop)\n",
    "        if np.abs(log_likelihood - prev_log_likelihood) < tol:\n",
    "            print(\"Converged!\")\n",
    "            break\n",
    "\n",
    "        prev_log_likelihood = log_likelihood\n",
    "\n",
    "    return mu, covariances, weights\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data (for demonstration purposes)\n",
    "    from sklearn.datasets import make_blobs\n",
    "    \n",
    "    X = dataset\n",
    "    \n",
    "    # Run Incremental EM\n",
    "    n_components = 3\n",
    "    mu, covariances, weights = incremental_em(X, n_components=n_components, max_iter=50)\n",
    "\n",
    "    print(\"Final means (mu):\")\n",
    "    print(mu)\n",
    "    print(\"Final covariances (Sigma):\")\n",
    "    print(covariances)\n",
    "    print(\"Final weights (pi):\")\n",
    "    print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from typing import List, Tuple\n",
    "\n",
    "class IncrementalGMM:\n",
    "    def __init__(self, n_components: int, dim: int, learning_rate: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize Incremental Gaussian Mixture Model\n",
    "        \n",
    "        Args:\n",
    "            n_components: Number of Gaussian components\n",
    "            dim: Dimensionality of the data\n",
    "            learning_rate: Learning rate for parameter updates\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.dim = dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weights = np.ones(n_components) / n_components\n",
    "        self.means = np.random.randn(n_components, dim)\n",
    "        self.covs = np.array([np.eye(dim) for _ in range(n_components)])\n",
    "        \n",
    "    def _compute_responsibilities(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute responsibilities (E-step) for a single data point\n",
    "        \n",
    "        Args:\n",
    "            x: Single data point of shape (dim,)\n",
    "            \n",
    "        Returns:\n",
    "            responsibilities: Array of shape (n_components,)\n",
    "        \"\"\"\n",
    "        densities = np.array([\n",
    "            multivariate_normal.pdf(x, mean=mean, cov=cov)\n",
    "            for mean, cov in zip(self.means, self.covs)\n",
    "        ])\n",
    "        \n",
    "        weighted_densities = self.weights * densities\n",
    "        sum_densities = weighted_densities.sum()\n",
    "        \n",
    "        if sum_densities == 0:\n",
    "            return np.ones(self.n_components) / self.n_components\n",
    "            \n",
    "        return weighted_densities / sum_densities\n",
    "    \n",
    "    def _update_parameters(self, x: np.ndarray, responsibilities: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update parameters (M-step) using a single data point\n",
    "        \n",
    "        Args:\n",
    "            x: Single data point of shape (dim,)\n",
    "            responsibilities: Array of shape (n_components,)\n",
    "        \"\"\"\n",
    "        for k in range(self.n_components):\n",
    "            resp_k = responsibilities[k]\n",
    "            \n",
    "            # Update weight\n",
    "            self.weights[k] = (1 - self.learning_rate) * self.weights[k] + \\\n",
    "                            self.learning_rate * resp_k\n",
    "            \n",
    "            # Update mean\n",
    "            diff_mean = resp_k * (x - self.means[k])\n",
    "            self.means[k] += self.learning_rate * diff_mean\n",
    "            \n",
    "            # Update covariance\n",
    "            diff_outer = np.outer(x - self.means[k], x - self.means[k])\n",
    "            diff_cov = resp_k * (diff_outer - self.covs[k])\n",
    "            self.covs[k] += self.learning_rate * diff_cov\n",
    "            \n",
    "        # Normalize weights\n",
    "        self.weights /= self.weights.sum()\n",
    "        \n",
    "        # Ensure numerical stability of covariances\n",
    "        for k in range(self.n_components):\n",
    "            self.covs[k] = (self.covs[k] + self.covs[k].T) / 2  # Ensure symmetry\n",
    "            min_eig = np.min(np.real(np.linalg.eigvals(self.covs[k])))\n",
    "            if min_eig < 1e-6:\n",
    "                self.covs[k] += (1e-6 - min_eig) * np.eye(self.dim)\n",
    "    \n",
    "    def partial_fit(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update the model with a single data point\n",
    "        \n",
    "        Args:\n",
    "            x: Single data point of shape (dim,)\n",
    "        \"\"\"\n",
    "        if x.shape[0] != self.dim:\n",
    "            raise ValueError(f\"Expected data point of dimension {self.dim}, got {x.shape[0]}\")\n",
    "        \n",
    "        # E-step\n",
    "        responsibilities = self._compute_responsibilities(x)\n",
    "        \n",
    "        # M-step\n",
    "        self._update_parameters(x, responsibilities)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, n_epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Fit the model to the data using multiple passes\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, dim)\n",
    "            n_epochs: Number of passes through the data\n",
    "        \"\"\"\n",
    "        if X.shape[1] != self.dim:\n",
    "            raise ValueError(f\"Expected data of dimension {self.dim}, got {X.shape[1]}\")\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Shuffle data\n",
    "            np.random.shuffle(X)\n",
    "            \n",
    "            # Process each point\n",
    "            for x in X:\n",
    "                self.partial_fit(x)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute responsibilities for multiple data points\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, dim)\n",
    "            \n",
    "        Returns:\n",
    "            responsibilities: Array of shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        return np.array([self._compute_responsibilities(x) for x in X])\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict cluster assignments for multiple data points\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, dim)\n",
    "            \n",
    "        Returns:\n",
    "            assignments: Array of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        return self.predict_proba(X).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "X = np.concatenate([\n",
    "    np.random.normal(0, 1, (n_samples // 2, 2)),\n",
    "    np.random.normal(4, 1.5, (n_samples // 2, 2))\n",
    "])\n",
    "\n",
    "# Initialize and fit the model\n",
    "gmm = IncrementalGMM(n_components=2, dim=2)\n",
    "gmm.fit(X, n_epochs=3)\n",
    "\n",
    "# Make predictions\n",
    "predictions = gmm.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
