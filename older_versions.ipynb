{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalEMGMM:\n",
    "    def __init__(self, n_components, n_dims):\n",
    "        \"\"\"\n",
    "        Initialize the Incremental EM for Gaussian Mixture Model.\n",
    "\n",
    "        Parameters:\n",
    "        - n_components: int, number of Gaussian components\n",
    "        - n_dims: int, dimensionality of the data\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_dims = n_dims\n",
    "        self.weights = np.ones(n_components) / n_components  # Mixing weights\n",
    "        self.means = np.random.rand(n_components, n_dims)    # Means of the Gaussians\n",
    "        self.covariances = np.array([np.eye(n_dims) for _ in range(n_components)])  # Covariances\n",
    "        self.total_samples = 0  # Total number of samples processed\n",
    "\n",
    "    def e_step(self, X):\n",
    "        \"\"\"\n",
    "        E-step: Compute responsibilities for each data point.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray, shape (batch_size, n_dims), batch of data\n",
    "\n",
    "        Returns:\n",
    "        - responsibilities: ndarray, shape (batch_size, n_components)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        responsibilities = np.zeros((batch_size, self.n_components))\n",
    "\n",
    "        # Compute the probability of each data point under each Gaussian component\n",
    "        for k in range(self.n_components):\n",
    "            rv = multivariate_normal(mean=self.means[k], cov=self.covariances[k])\n",
    "            responsibilities[:, k] = self.weights[k] * rv.pdf(X)\n",
    "\n",
    "        # Normalize the responsibilities across all components\n",
    "        responsibilities_sum = responsibilities.sum(axis=1)[:, np.newaxis]\n",
    "        responsibilities /= responsibilities_sum\n",
    "\n",
    "        return responsibilities\n",
    "\n",
    "    def m_step(self, X, responsibilities):\n",
    "        \"\"\"\n",
    "        M-step: Update model parameters using responsibilities.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray, shape (batch_size, n_dims), batch of data\n",
    "        - responsibilities: ndarray, shape (batch_size, n_components)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        Nk = responsibilities.sum(axis=0)  # Effective number of samples for each component\n",
    "\n",
    "        # Update weights\n",
    "        self.weights = ((self.total_samples * self.weights) + Nk) / (self.total_samples + batch_size)\n",
    "\n",
    "        # Update means and covariances incrementally\n",
    "        for k in range(self.n_components):\n",
    "            resp = responsibilities[:, k][:, np.newaxis]\n",
    "            sum_resp = resp.sum()\n",
    "\n",
    "            # Update means\n",
    "            mean_update = (resp * X).sum(axis=0)\n",
    "            self.means[k] = ((self.total_samples * self.means[k]) + mean_update) / (self.total_samples + sum_resp)\n",
    "\n",
    "            # Update covariances\n",
    "            diff = X - self.means[k]\n",
    "            cov_update = np.dot((resp * diff).T, diff)\n",
    "            self.covariances[k] = ((self.total_samples * self.covariances[k]) + cov_update) / (self.total_samples + sum_resp)\n",
    "\n",
    "        self.total_samples += batch_size\n",
    "\n",
    "    def fit(self, X_batches, n_epochs=1):\n",
    "        \"\"\"\n",
    "        Fit the model to the data incrementally.\n",
    "\n",
    "        Parameters:\n",
    "        - X_batches: list of ndarrays, each ndarray is a batch of data\n",
    "        - n_epochs: int, number of times to iterate over the batches\n",
    "        \"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            for X in X_batches:\n",
    "                responsibilities = self.e_step(X)\n",
    "                self.m_step(X, responsibilities)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute the posterior probabilities for each component given the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray, shape (n_samples, n_dims), input data\n",
    "\n",
    "        Returns:\n",
    "        - probabilities: ndarray, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        return self.e_step(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Assign each data point to the component with the highest posterior probability.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray, shape (n_samples, n_dims), input data\n",
    "\n",
    "        Returns:\n",
    "        - labels: ndarray, shape (n_samples,), component assignments\n",
    "        \"\"\"\n",
    "        responsibilities = self.e_step(X)\n",
    "        return np.argmax(responsibilities, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Means:\n",
      "[[0.95493457 2.69975487]\n",
      " [4.54172057 4.55038648]\n",
      " [0.93569719 2.68721811]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_dims = 2\n",
    "    n_components = 3\n",
    "    n_samples = 1000\n",
    "\n",
    "    true_means = np.array([[0, 0], [5, 5], [0, 5]])\n",
    "    true_covs = np.array([np.eye(2), np.eye(2), np.eye(2)])\n",
    "    true_weights = np.array([0.3, 0.5, 0.2])\n",
    "\n",
    "    # Generate samples from the true GMM\n",
    "    X = np.vstack([\n",
    "        np.random.multivariate_normal(true_means[k], true_covs[k], size=int(true_weights[k]*n_samples))\n",
    "        for k in range(n_components)\n",
    "    ])\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    # Split data into mini-batches\n",
    "    batch_size = 100\n",
    "    X_batches = [X[i:i + batch_size] for i in range(0, n_samples, batch_size)]\n",
    "\n",
    "    # Initialize and fit the incremental EM GMM\n",
    "    incremental_gmm = IncrementalEMGMM(n_components=n_components, n_dims=n_dims)\n",
    "    incremental_gmm.fit(X_batches, n_epochs=5)\n",
    "\n",
    "    # Predict component assignments\n",
    "    labels = incremental_gmm.predict(X)\n",
    "\n",
    "    # Print the estimated means\n",
    "    print(\"Estimated Means:\")\n",
    "    print(incremental_gmm.means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Means:\n",
      "[[ 4.75037457  4.54113817]\n",
      " [-0.00788832  2.03898053]\n",
      " [ 5.11664584  5.18037287]]\n",
      "\n",
      "True Means:\n",
      "[[0 0]\n",
      " [5 5]\n",
      " [0 5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class OnlineEMGMM:\n",
    "    def __init__(self, n_components, n_dims, t0=10, kappa=0.6):\n",
    "        \"\"\"\n",
    "        Initialize the Online EM for Gaussian Mixture Model.\n",
    "\n",
    "        Parameters:\n",
    "        - n_components: int, number of Gaussian components\n",
    "        - n_dims: int, dimensionality of the data\n",
    "        - t0: float, delay parameter for learning rate schedule\n",
    "        - kappa: float, forgetting factor (0.5 < kappa <= 1)\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_dims = n_dims\n",
    "        self.t = 0  # Time step (number of samples processed)\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.weights = np.ones(n_components) / n_components  # Mixing weights π_k\n",
    "        self.means = np.random.randn(n_components, n_dims)   # Means μ_k\n",
    "        self.covariances = np.array([np.eye(n_dims) for _ in range(n_components)])  # Covariances Σ_k\n",
    "\n",
    "        # Initialize sufficient statistics\n",
    "        self.s0 = np.zeros(n_components)  # Sum of responsibilities for each component\n",
    "        self.s1 = np.zeros((n_components, n_dims))  # Sum of weighted data points\n",
    "        self.s2 = np.zeros((n_components, n_dims, n_dims))  # Sum of weighted outer products\n",
    "\n",
    "        # Learning rate schedule parameters\n",
    "        self.t0 = t0\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def compute_responsibilities(self, x):\n",
    "        \"\"\"\n",
    "        E-step: Compute responsibilities for a single data point.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data sample\n",
    "\n",
    "        Returns:\n",
    "        - responsibilities: ndarray, shape (n_components,), responsibilities for each component\n",
    "        \"\"\"\n",
    "        responsibilities = np.zeros(self.n_components)\n",
    "        for k in range(self.n_components):\n",
    "            rv = multivariate_normal(mean=self.means[k], cov=self.covariances[k], allow_singular=True)\n",
    "            responsibilities[k] = self.weights[k] * rv.pdf(x)\n",
    "        total = responsibilities.sum()\n",
    "        if total == 0:\n",
    "            # Avoid division by zero\n",
    "            responsibilities = np.ones(self.n_components) / self.n_components\n",
    "        else:\n",
    "            responsibilities /= total\n",
    "        return responsibilities\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        Update the model parameters with a single data point.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data sample\n",
    "        \"\"\"\n",
    "        self.t += 1  # Increment time step\n",
    "        alpha_t = (self.t0 + self.t) ** -self.kappa  # Learning rate\n",
    "\n",
    "        # E-step: Compute responsibilities\n",
    "        gamma = self.compute_responsibilities(x)\n",
    "\n",
    "        # M-step: Update sufficient statistics\n",
    "        for k in range(self.n_components):\n",
    "            # Update sufficient statistics with forgetting factor\n",
    "            self.s0[k] = (1 - alpha_t) * self.s0[k] + alpha_t * gamma[k]\n",
    "            self.s1[k] = (1 - alpha_t) * self.s1[k] + alpha_t * gamma[k] * x\n",
    "            x_diff = x - self.means[k]\n",
    "            outer_product = np.outer(x_diff, x_diff)\n",
    "            self.s2[k] = (1 - alpha_t) * self.s2[k] + alpha_t * gamma[k] * outer_product\n",
    "\n",
    "            # Update parameters\n",
    "            if self.s0[k] > 1e-6:  # Avoid division by zero\n",
    "                self.weights[k] = self.s0[k] / self.s0.sum()\n",
    "                self.means[k] = self.s1[k] / self.s0[k]\n",
    "                cov_matrix = self.s2[k] / self.s0[k]\n",
    "                cov_matrix = cov_matrix + 1e-6 * np.eye(self.n_dims)  # Regularization\n",
    "                self.covariances[k] = cov_matrix\n",
    "\n",
    "    def fit(self, X_stream):\n",
    "        \"\"\"\n",
    "        Fit the model to the data stream.\n",
    "\n",
    "        Parameters:\n",
    "        - X_stream: iterable of ndarrays, each ndarray is a data sample\n",
    "        \"\"\"\n",
    "        for x in X_stream:\n",
    "            self.update(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        Compute the posterior probabilities for each component given a data point.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data sample\n",
    "\n",
    "        Returns:\n",
    "        - probabilities: ndarray, shape (n_components,)\n",
    "        \"\"\"\n",
    "        return self.compute_responsibilities(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Assign a data point to the component with the highest posterior probability.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data sample\n",
    "\n",
    "        Returns:\n",
    "        - label: int, component assignment\n",
    "        \"\"\"\n",
    "        responsibilities = self.compute_responsibilities(x)\n",
    "        return np.argmax(responsibilities)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_dims = 2\n",
    "    n_components = 3\n",
    "    n_samples = 1000\n",
    "\n",
    "    true_means = np.array([[0, 0], [5, 5], [0, 5]])\n",
    "    true_covs = np.array([np.eye(2), np.eye(2), np.eye(2)])\n",
    "    true_weights = np.array([0.3, 0.5, 0.2])\n",
    "\n",
    "    # Generate samples from the true GMM\n",
    "    X = np.vstack([\n",
    "        np.random.multivariate_normal(true_means[k], true_covs[k], size=int(true_weights[k]*n_samples))\n",
    "        for k in range(n_components)\n",
    "    ])\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    # Initialize the online EM GMM\n",
    "    online_gmm = OnlineEMGMM(n_components=n_components, n_dims=n_dims)\n",
    "\n",
    "    # Simulate data arriving one by one\n",
    "    for x in X:\n",
    "        online_gmm.update(x)\n",
    "\n",
    "    # After processing all data, print the estimated means\n",
    "    print(\"Estimated Means:\")\n",
    "    print(online_gmm.means)\n",
    "\n",
    "    # Predict component assignments for all data\n",
    "    labels = [online_gmm.predict(x) for x in X]\n",
    "\n",
    "    # Optionally, compare the estimated parameters with the true parameters\n",
    "    print(\"\\nTrue Means:\")\n",
    "    print(true_means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Means:\n",
      "[[ 0.09730099 -0.04656867]\n",
      " [ 3.41813348  5.11451274]\n",
      " [ 0.09210458 -0.05114168]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class OnlineEMGMM:\n",
    "    def __init__(self, n_components, n_dims, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Online EM for Gaussian Mixture Model.\n",
    "\n",
    "        Parameters:\n",
    "        - n_components: int, number of Gaussian components\n",
    "        - n_dims: int, dimensionality of the data\n",
    "        - learning_rate: float, controls the rate of updating\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_dims = n_dims\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.weights = np.ones(n_components) / n_components  # Mixing weights\n",
    "        self.means = np.random.rand(n_components, n_dims)    # Means of the Gaussians\n",
    "        self.covariances = np.array([np.eye(n_dims) for _ in range(n_components)])  # Covariances\n",
    "\n",
    "    def e_step(self, x):\n",
    "        \"\"\"\n",
    "        E-step: Compute responsibilities for a single data point.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data point\n",
    "\n",
    "        Returns:\n",
    "        - responsibilities: ndarray, shape (n_components,)\n",
    "        \"\"\"\n",
    "        responsibilities = np.zeros(self.n_components)\n",
    "\n",
    "        # Compute the probability of the data point under each Gaussian component\n",
    "        for k in range(self.n_components):\n",
    "            rv = multivariate_normal(mean=self.means[k], cov=self.covariances[k])\n",
    "            responsibilities[k] = self.weights[k] * rv.pdf(x)\n",
    "\n",
    "        # Normalize responsibilities\n",
    "        responsibilities /= responsibilities.sum()\n",
    "\n",
    "        return responsibilities\n",
    "\n",
    "    def m_step(self, x, responsibilities):\n",
    "        \"\"\"\n",
    "        M-step: Update model parameters for a single data point.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data point\n",
    "        - responsibilities: ndarray, shape (n_components,), responsibilities for the data point\n",
    "        \"\"\"\n",
    "        for k in range(self.n_components):\n",
    "            # Learning rate\n",
    "            lr = self.learning_rate * responsibilities[k]\n",
    "\n",
    "            # Update weights\n",
    "            self.weights[k] = (1 - self.learning_rate) * self.weights[k] + self.learning_rate * responsibilities[k]\n",
    "\n",
    "            # Update means\n",
    "            self.means[k] = (1 - lr) * self.means[k] + lr * x\n",
    "\n",
    "            # Update covariances\n",
    "            diff = (x - self.means[k]).reshape(-1, 1)\n",
    "            self.covariances[k] = (1 - lr) * self.covariances[k] + lr * np.dot(diff, diff.T)\n",
    "\n",
    "        # Normalize weights\n",
    "        self.weights /= self.weights.sum()\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"\n",
    "        Process a single data point and update the model parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data point\n",
    "        \"\"\"\n",
    "        responsibilities = self.e_step(x)\n",
    "        self.m_step(x, responsibilities)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        Compute the posterior probabilities for a single data point.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data point\n",
    "\n",
    "        Returns:\n",
    "        - responsibilities: ndarray, shape (n_components,)\n",
    "        \"\"\"\n",
    "        return self.e_step(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Assign the data point to the component with the highest posterior probability.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (n_dims,), a single data point\n",
    "\n",
    "        Returns:\n",
    "        - label: int, the component assignment\n",
    "        \"\"\"\n",
    "        responsibilities = self.e_step(x)\n",
    "        return np.argmax(responsibilities)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_dims = 2\n",
    "    n_components = 3\n",
    "    n_samples = 1000\n",
    "\n",
    "    true_means = np.array([[0, 0], [5, 5], [0, 5]])\n",
    "    true_covs = np.array([np.eye(2), np.eye(2), np.eye(2)])\n",
    "    true_weights = np.array([0.3, 0.5, 0.2])\n",
    "\n",
    "    # Generate samples from the true GMM\n",
    "    X = np.vstack([\n",
    "        np.random.multivariate_normal(true_means[k], true_covs[k], size=int(true_weights[k] * n_samples))\n",
    "        for k in range(n_components)\n",
    "    ])\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    # Initialize the online EM GMM\n",
    "    online_gmm = OnlineEMGMM(n_components=n_components, n_dims=n_dims, learning_rate=0.05)\n",
    "\n",
    "    # Process each data point one by one\n",
    "    for x in X:\n",
    "        online_gmm.update(x)\n",
    "\n",
    "    # Print the estimated means\n",
    "    print(\"Estimated Means:\")\n",
    "    print(online_gmm.means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Means:\n",
      "[[ 3.03238479  2.57738491]\n",
      " [-0.04135715 -0.05512272]\n",
      " [ 3.49319566  5.17840849]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class IncrementalEMGMM:\n",
    "    def __init__(self, n_components, n_dims):\n",
    "        \"\"\"\n",
    "        Initialize the Incremental EM for Gaussian Mixture Model.\n",
    "\n",
    "        Parameters:\n",
    "        - n_components: int, number of Gaussian components\n",
    "        - n_dims: int, dimensionality of the data\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "        # Initialize parameters θ(µ)\n",
    "        self.weights = np.ones(n_components) / n_components  # Mixing weights\n",
    "        self.means = np.random.rand(n_components, n_dims)    # Means of the Gaussians\n",
    "        self.covariances = np.array([np.eye(n_dims) for _ in range(n_components)])  # Covariances\n",
    "\n",
    "        # Initialize s_i for all data points i\n",
    "        self.s_i_list = None  # Will be initialized in fit method\n",
    "\n",
    "        # Initialize µ = sum_i s_i (sufficient statistics)\n",
    "        self.N_k = np.zeros(n_components)  # Component counts\n",
    "        self.Sum_k = np.zeros((n_components, n_dims))  # Sum of x_i for each component\n",
    "        self.Sum_squares_k = np.zeros((n_components, n_dims, n_dims))  # Sum of x_i x_i^T for each component\n",
    "\n",
    "    def initialize_s_i(self, X):\n",
    "        \"\"\"\n",
    "        Initialize s_i for each data point i.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray, shape (n_samples, n_dims), the data\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        self.s_i_list = []\n",
    "\n",
    "        # Random initial responsibilities\n",
    "        for i in range(n_samples):\n",
    "            gamma_ik = np.random.dirichlet(np.ones(self.n_components))\n",
    "\n",
    "            s_i_k = []\n",
    "            x_i = X[i]\n",
    "            for k in range(self.n_components):\n",
    "                gamma = gamma_ik[k]\n",
    "                s_i_k.append({\n",
    "                    'gamma': gamma,\n",
    "                    'gamma_x': gamma * x_i,\n",
    "                    'gamma_xxT': gamma * np.outer(x_i, x_i)\n",
    "                })\n",
    "\n",
    "                # Update µ\n",
    "                self.N_k[k] += gamma\n",
    "                self.Sum_k[k] += gamma * x_i\n",
    "                self.Sum_squares_k[k] += gamma * np.outer(x_i, x_i)\n",
    "\n",
    "            self.s_i_list.append(s_i_k)\n",
    "\n",
    "        # Update θ(µ)\n",
    "        self.update_parameters()\n",
    "\n",
    "    def update_parameters(self):\n",
    "        \"\"\"\n",
    "        Update model parameters θ(µ) based on the current µ (sufficient statistics).\n",
    "        \"\"\"\n",
    "        total_N = np.sum(self.N_k)\n",
    "        self.weights = self.N_k / total_N\n",
    "        self.means = self.Sum_k / self.N_k[:, np.newaxis]\n",
    "        self.covariances = np.zeros((self.n_components, self.n_dims, self.n_dims))\n",
    "\n",
    "        for k in range(self.n_components):\n",
    "            diff = self.Sum_squares_k[k] / self.N_k[k] - np.outer(self.means[k], self.means[k])\n",
    "            self.covariances[k] = diff + np.eye(self.n_dims) * 1e-6  # Regularization\n",
    "\n",
    "    def fit(self, X, n_iterations=10):\n",
    "        \"\"\"\n",
    "        Fit the model to the data incrementally.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray, shape (n_samples, n_dims), the data\n",
    "        - n_iterations: int, number of EM iterations\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Initialize s_i for each data point\n",
    "        self.initialize_s_i(X)\n",
    "\n",
    "        # For each iteration\n",
    "        for iteration in range(n_iterations):\n",
    "            # Process data points in random order\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            for idx in indices:\n",
    "                x_i = X[idx]\n",
    "                s_i_old = self.s_i_list[idx]\n",
    "                s_i_new = []\n",
    "\n",
    "                # Compute responsibilities gamma_ik = p(z=k | x_i; θ(µ))\n",
    "                gamma_ik = np.zeros(self.n_components)\n",
    "                for k in range(self.n_components):\n",
    "                    rv = multivariate_normal(mean=self.means[k], cov=self.covariances[k])\n",
    "                    gamma_ik[k] = self.weights[k] * rv.pdf(x_i)\n",
    "                gamma_ik /= np.sum(gamma_ik)\n",
    "\n",
    "                # Compute s'_i\n",
    "                for k in range(self.n_components):\n",
    "                    gamma = gamma_ik[k]\n",
    "                    s_i_k = {\n",
    "                        'gamma': gamma,\n",
    "                        'gamma_x': gamma * x_i,\n",
    "                        'gamma_xxT': gamma * np.outer(x_i, x_i)\n",
    "                    }\n",
    "                    s_i_new.append(s_i_k)\n",
    "\n",
    "                    # Update µ ← µ + s'_i_k - s_i_old_k\n",
    "                    self.N_k[k] += s_i_k['gamma'] - s_i_old[k]['gamma']\n",
    "                    self.Sum_k[k] += s_i_k['gamma_x'] - s_i_old[k]['gamma_x']\n",
    "                    self.Sum_squares_k[k] += s_i_k['gamma_xxT'] - s_i_old[k]['gamma_xxT']\n",
    "\n",
    "                # Replace s_i ← s'_i\n",
    "                self.s_i_list[idx] = s_i_new\n",
    "\n",
    "            # Update parameters after each iteration\n",
    "            self.update_parameters()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute the posterior probabilities for each component given the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray, shape (n_samples, n_dims), input data\n",
    "\n",
    "        Returns:\n",
    "        - probabilities: ndarray, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_i = X[i]\n",
    "            for k in range(self.n_components):\n",
    "                rv = multivariate_normal(mean=self.means[k], cov=self.covariances[k])\n",
    "                responsibilities[i, k] = self.weights[k] * rv.pdf(x_i)\n",
    "            responsibilities[i, :] /= np.sum(responsibilities[i, :])\n",
    "\n",
    "        return responsibilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Assign each data point to the component with the highest posterior probability.\n",
    "\n",
    "        Parameters:\n",
    "        - X: ndarray, shape (n_samples, n_dims), input data\n",
    "\n",
    "        Returns:\n",
    "        - labels: ndarray, shape (n_samples,), component assignments\n",
    "        \"\"\"\n",
    "        responsibilities = self.predict_proba(X)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_dims = 2\n",
    "    n_components = 3\n",
    "    n_samples = 1000\n",
    "\n",
    "    true_means = np.array([[0, 0], [5, 5], [0, 5]])\n",
    "    true_covs = np.array([np.eye(2), np.eye(2), np.eye(2)])\n",
    "    true_weights = np.array([0.3, 0.5, 0.2])\n",
    "\n",
    "    # Generate samples from the true GMM\n",
    "    X = np.vstack([\n",
    "        np.random.multivariate_normal(true_means[k], true_covs[k], size=int(true_weights[k]*n_samples))\n",
    "        for k in range(n_components)\n",
    "    ])\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(X)\n",
    "\n",
    "    # Initialize and fit the incremental EM GMM\n",
    "    incremental_gmm = IncrementalEMGMM(n_components=n_components, n_dims=n_dims)\n",
    "    incremental_gmm.fit(X, n_iterations=20)\n",
    "\n",
    "    # Predict component assignments\n",
    "    labels = incremental_gmm.predict(X)\n",
    "\n",
    "    # Print the estimated means\n",
    "    print(\"\\nEstimated Means:\")\n",
    "    print(incremental_gmm.means)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
