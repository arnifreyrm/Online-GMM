{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_components, n_features, first_points=None, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Initialize mixing coefficients\n",
    "    pi = np.ones(n_components) / n_components\n",
    "    \n",
    "    # Initialize means with more spread\n",
    "    if first_points is not None:\n",
    "        mu_k = first_points + np.random.randn(n_components, n_features) * 3.0\n",
    "    else:\n",
    "        mu_k = np.random.randn(n_components, n_features) * 5.0\n",
    "    \n",
    "    # Initialize with larger covariances\n",
    "    sigma_k = np.array([\n",
    "        np.eye(n_features) * (2.0 + 0.5 * k) \n",
    "        for k in range(n_components)\n",
    "    ])\n",
    "    \n",
    "    return pi, mu_k, sigma_k\n",
    "\n",
    "def compute_responsibility(x_i, pi, mu_k, sigma_k):\n",
    "    n_components = len(pi)\n",
    "    probs = np.zeros(n_components)\n",
    "    \n",
    "    # Calculate distances to each center\n",
    "    distances = np.array([np.sum((x_i - mu_k[k])**2) for k in range(n_components)])\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        try:\n",
    "            probs[k] = pi[k] * multivariate_normal.pdf(x_i, mean=mu_k[k], cov=sigma_k[k])\n",
    "        except np.linalg.LinAlgError:\n",
    "            sigma_k[k] = np.eye(len(x_i)) * np.trace(sigma_k[k]) / len(x_i)\n",
    "            probs[k] = pi[k] * multivariate_normal.pdf(x_i, mean=mu_k[k], cov=sigma_k[k])\n",
    "    \n",
    "    # Add small constant for numerical stability\n",
    "    probs += 1e-10\n",
    "    \n",
    "    # Sharpen the responsibilities based on distances\n",
    "    gamma = probs / probs.sum()\n",
    "    \n",
    "    # Make responsibilities more extreme\n",
    "    closest_center = np.argmin(distances)\n",
    "    gamma = gamma ** 2  # Square to make differences more extreme\n",
    "    gamma = gamma / gamma.sum()  # Renormalize\n",
    "    \n",
    "    # If point is very close to a center, assign it mostly to that center\n",
    "    min_dist = np.min(distances)\n",
    "    if min_dist < 1.0:  # Threshold distance\n",
    "        gamma *= 0.1  # Reduce all responsibilities\n",
    "        gamma[closest_center] = 0.9  # Give high responsibility to closest center\n",
    "    \n",
    "    return gamma\n",
    "\n",
    "def update_parameters(x_i, gamma, pi, mu_k, sigma_k, N_k, step_size, t):\n",
    "    n_components = len(pi)\n",
    "    n_features = len(x_i)\n",
    "    min_covar = 1e-6\n",
    "    \n",
    "    # Update counts with momentum\n",
    "    N_k = (1 - step_size) * N_k + step_size * gamma\n",
    "    \n",
    "    # Update mixing coefficients\n",
    "    pi = N_k / N_k.sum()\n",
    "    \n",
    "    # Compute adaptive rates for each component based on counts in each cluster\n",
    "    adaptive_rates = np.clip(1.0 / (N_k + 1e-10), 0, 1)\n",
    "    \n",
    "    # Update means with adaptive rates\n",
    "    for k in range(n_components):\n",
    "        delta = x_i - mu_k[k]\n",
    "        effective_rate = step_size \\\n",
    "            * adaptive_rates[k]\n",
    "        # effective_rate = step_size\n",
    "        \n",
    "        # More aggressive update for components with high responsibility\n",
    "        if gamma[k] > 0.5:  # If component has high responsibility\n",
    "            effective_rate *= 2.0\n",
    "        \n",
    "        mu_k[k] += effective_rate * gamma[k] * delta\n",
    "    \n",
    "    # Update covariances\n",
    "    for k in range(n_components):\n",
    "        delta = x_i - mu_k[k]\n",
    "        delta_out = np.outer(delta, delta)\n",
    "        # effective_rate = step_size * adaptive_rates[k]\n",
    "        effective_rate = step_size \\\n",
    "            * adaptive_rates[k]\n",
    "        \n",
    "        sigma_k[k] = (1 - effective_rate) * sigma_k[k] + \\\n",
    "                     effective_rate * gamma[k] * delta_out\n",
    "        \n",
    "        # Ensure positive definiteness\n",
    "        sigma_k[k] = (sigma_k[k] + sigma_k[k].T) / 2\n",
    "        eigvals, eigvecs = np.linalg.eigh(sigma_k[k])\n",
    "        eigvals = np.maximum(eigvals, min_covar)\n",
    "        sigma_k[k] = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "    \n",
    "    return pi, mu_k, sigma_k, N_k\n",
    "\n",
    "def online_gmm(data_generator, n_components, n_features, n_iterations, alpha=0.6, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Collect first few points for initialization\n",
    "    first_points = np.array([next(data_generator) for _ in range(n_components)])\n",
    "    # first_points = None  # Disable initialization with first points\n",
    "\n",
    "    # Initialize parameters\n",
    "    pi, mu_k, sigma_k = initialize_parameters(n_components, n_features, first_points, random_state)\n",
    "    N_k = np.ones(n_components) / n_components\n",
    "    \n",
    "    # Store history for visualization\n",
    "    history_mu = []\n",
    "    \n",
    "    # Process data points one at a time\n",
    "    for t in range(n_iterations):\n",
    "        x_i = next(data_generator)\n",
    "        \n",
    "        # Adaptive step size with slower decay\n",
    "        step_size = (t + 2) ** -alpha\n",
    "        \n",
    "        # E-step: Compute responsibilities\n",
    "        gamma = compute_responsibility(x_i, pi, mu_k, sigma_k)\n",
    "        \n",
    "        # M-step: Update parameters\n",
    "        pi, mu_k, sigma_k, N_k = update_parameters(x_i, gamma, pi, mu_k, sigma_k, N_k, step_size, t)\n",
    "        \n",
    "        # Store current means\n",
    "        history_mu.append(mu_k.copy())\n",
    "        \n",
    "        # Prevent centers from collapsing or getting stuck\n",
    "        # if t % 50 == 0:  # Check more frequently\n",
    "        #     distances = np.sqrt(((mu_k[:, None] - mu_k[None, :]) ** 2).sum(axis=2))\n",
    "        #     np.fill_diagonal(distances, np.inf)\n",
    "            \n",
    "        #     # If centers are too close or one is near the mean\n",
    "        #     if (distances < 0.5).any():\n",
    "        #         # Add larger random perturbation to centers\n",
    "        #         mu_k += np.random.randn(*mu_k.shape) * 0.5\n",
    "    \n",
    "    return pi, mu_k, sigma_k, history_mu\n",
    "\n",
    "def data_generator(X):\n",
    "    idx = 0\n",
    "    n_samples = len(X)\n",
    "    while idx < n_samples:\n",
    "        yield X[idx]\n",
    "        idx += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data with more separated clusters\n",
    "    n_samples = 20000\n",
    "    n_components = 4\n",
    "    n_features = 2\n",
    "    X, y_true = make_blobs(n_samples=n_samples, centers=n_components, \n",
    "                          cluster_std=0.80, random_state=1126)\n",
    "    \n",
    "    # Create data generator\n",
    "    X = shuffle(X, random_state=0)\n",
    "    gen = data_generator(X)\n",
    "    \n",
    "    # Run online GMM\n",
    "    n_iterations = n_samples - n_components\n",
    "    alpha = 0.75\n",
    "    random_state = 1126\n",
    "    \n",
    "    pi, mu_k, sigma_k, history_mu = online_gmm(gen, n_components, n_features, \n",
    "                                              n_iterations, alpha, random_state)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c='gray', s=40, alpha=0.2, label='Data Points')\n",
    "    \n",
    "    # Plot final cluster centers\n",
    "    plt.scatter(mu_k[:, 0], mu_k[:, 1], c='red', s=200, marker='X', label='Final Centers')\n",
    "    \n",
    "    # Plot center trajectories\n",
    "    history_mu = np.array(history_mu)\n",
    "    for k in range(n_components):\n",
    "        plt.plot(history_mu[:, k, 0], history_mu[:, k, 1], 'k-', alpha=0.3)\n",
    "    \n",
    "    plt.title('Online Gaussian Mixture Model')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
